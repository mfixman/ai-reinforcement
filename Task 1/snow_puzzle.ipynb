{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice Puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](snow_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install packages\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment:\n",
    "    def __init__(self,start,goal,length=10,height=5,punishment=-5,reward=100):\n",
    "        # Define environment size\n",
    "        self.length=length\n",
    "        self.height=height\n",
    "        self.Map=np.array(np.zeros((height,length)))\n",
    "\n",
    "        # Define Items on the Map\n",
    "        self.start=start #Starting Point\n",
    "        self.goal=goal\n",
    "        self.punishment=punishment\n",
    "        self.reward=reward\n",
    "\n",
    "        # Define Variables for Training\n",
    "        self.actions_map = {\"Up\": 0,\"Down\": 1,\"Left\": 2,\"Right\": 3}\n",
    "        self.Q=None\n",
    "        self.transitions = dict()\n",
    "        self.R = dict()\n",
    "        self.S = set()\n",
    "        self.episode_actions=[]\n",
    "\n",
    "        # Define the next state because this map cannot utilise simple directions\n",
    "        self.transitions=None\n",
    "\n",
    "    def GiveTransition(self,loc,act,destination):\n",
    "        # Variables:\n",
    "        # 1) loc: Coordinate in the form of 'x,y' INCLUDING the apostrophes, \n",
    "        # which converts to its corresponding index through self.s_dict\n",
    "        # 2) acts: Possible actions in the form of ['Up','Down,'Left','Right'] where any of the directions \n",
    "        # can be removed to indicate the action is not possible\n",
    "        # 3) destinations: End coordinate after taking the action defined above in the form of ['x,y']\n",
    "        # Shapes of acts and destinations must be the same\n",
    "\n",
    "        self.transitions[loc,self.actions_map[act]] = destination\n",
    "        self.R[loc,self.actions_map[act]] = 0\n",
    "        self.S.insert(loc)\n",
    "\n",
    "    def CreateQMat(self):\n",
    "        self.Q=np.zeros((len(self.St),len(self.actions)))\n",
    "\n",
    "    #Assign reward value of punishment value\n",
    "    def GiveValue(self,loc,acts,value=None): \n",
    "        if value is None:\n",
    "            value = self.reward\n",
    "        self.R[loc,self.actions.index(acts)]=value\n",
    "\n",
    "    def TrainEpisode(self,alpha,gamma,epsilon,max_step,default_start=True): \n",
    "        #...\n",
    "        # Training Episodes using Code from INM707 Lab 4 as reference\n",
    "        #...\n",
    "        if(default_start==True):\n",
    "            curr_s=self.start\n",
    "        else:\n",
    "            curr_s=random.randint(0,len(self.S)-1)\n",
    "        # print(\"Starting state is '{}'\".format(list(self.S_dict.keys())[curr_s]))\n",
    "        self.episode_actions=[]\n",
    "        for step in range(max_step):\n",
    "\n",
    "            # Define actions for both exploring and exploiting policies\n",
    "            open_actions = np.where(~np.isnan(self.R[curr_s]))[0]\n",
    "            # print([self.actions[a] for a in open_actions])\n",
    "\n",
    "            open_q = [self.Q[curr_s,a] for a in open_actions]\n",
    "\n",
    "            best_act = open_actions[np.where(open_q == np.max(open_q))[0]]\n",
    "            best_act_q = [self.Q[curr_s,x] for x in best_act]\n",
    "\n",
    "            # print(best_act)\n",
    "\n",
    "            # Pick Action based on policy\n",
    "            if np.random.uniform() < epsilon:\n",
    "                a = np.random.choice(open_actions)\n",
    "            else:\n",
    "                a = np.random.choice(best_act)\n",
    "\n",
    "            # Update Environment States\n",
    "            r = self.R[curr_s,a]\n",
    "            s_old = curr_s\n",
    "            curr_s = int(self.transitions[curr_s,a])\n",
    "\n",
    "            self.episode_actions.append(\"{}, {}\".format(self.S[s_old],self.actions[a]))\n",
    "            # print(\"New state is '{}'\".format(list(self.S_dict.keys())[int(curr_s)]))\n",
    "            # print((self.Q[curr_s]))\n",
    "            q_updated =  self.Q[s_old,a] + alpha*(self.R[s_old,a] + gamma*np.max(self.Q[curr_s]) - self.Q[s_old,a])\n",
    "            self.Q[s_old,a] = q_updated\n",
    "\n",
    "            # print('Q matrix updated: \\n\\n {}'.format(self.Q.round(0)))\n",
    "\n",
    "            if curr_s == self.goal:\n",
    "                # print(\"Goal state '{}' reached. Ending episode.\".format(self.goal))\n",
    "                break\n",
    "\n",
    "        return self.episode_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan   0.  nan   0.   0.  -1.   0.  -1.   0.  nan]\n",
      " [ nan   0.   0.  -1.  -1.  -1.  -1.  -1.  -1. 100.]\n",
      " [ nan  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  nan]\n",
      " [  0.   0.  -1.  -1.  -1.  -1.   0.  nan  -1.  nan]\n",
      " [  0.  nan   0.   0.   0.  nan   0.  -1.   0.  nan]]\n"
     ]
    }
   ],
   "source": [
    "#Define Totems (Tiles that cant be stood on), Ice (Tiles that can only be slid past), Treasure(Tile containing reward) and Start(Starting Point)\n",
    "Totems=[(0,0),(1,0),(2,0),(0,2),(4,1),(4,5),(3,7),(0,9),(2,9),(3,9),(4,9)]\n",
    "Ice=[(2,1),(2,2),(3,2),(1,3),(2,3),(3,3),(1,4),(2,4),(3,4),(0,5),(1,5),(2,5),(3,5),(2,7),(2,8),(1,6),(2,6),(0,7),(1,7),(4,7),(1,8),(3,8)]\n",
    "Treasure=[(1,9)]\n",
    "Start='3,0'\n",
    "End = '1,9']\n",
    "Start='3,0'\n",
    "End = '1,9'\n",
    "\n",
    "Map = IceEnv.CreateMap()\n",
    "print(Map)\n",
    "\n",
    "#Create State Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read map from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('snow_map').readlines()\n",
    "\n",
    "# Remove first line\n",
    "lines = lines[1:]\n",
    "\n",
    "map = []\n",
    "for line in lines:\n",
    "    map.append(line[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 0123456789\n",
    "0x x      x\n",
    "1x        r\n",
    "2         x\n",
    "3       x  \n",
    "4     x   x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'4,1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m dx\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;241m!=\u001b[39m i \u001b[38;5;129;01mor\u001b[39;00m x \u001b[38;5;241m!=\u001b[39m j:\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mIceEnv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGiveTransition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mj\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43my\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmap\u001b[39m[y][x] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     29\u001b[0m         IceEnv\u001b[38;5;241m.\u001b[39mGiveValue(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mdir\u001b[39m)\n",
      "Cell \u001b[0;32mIn[50], line 38\u001b[0m, in \u001b[0;36mEnvironment.GiveTransition\u001b[0;34m(self, loc, act, destination)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGiveTransition\u001b[39m(\u001b[38;5;28mself\u001b[39m,loc,act,destination):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Variables:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# 1) loc: Coordinate in the form of 'x,y' INCLUDING the apostrophes, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# 3) destinations: End coordinate after taking the action defined above in the form of ['x,y']\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Shapes of acts and destinations must be the same\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransitions[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS_dict[loc],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions_map[act]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS_dict[loc],\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions_map[act]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS\u001b[38;5;241m.\u001b[39minsert(loc)\n",
      "\u001b[0;31mKeyError\u001b[0m: '4,1'"
     ]
    }
   ],
   "source": [
    "N = len(map)\n",
    "M = len(map[0])\n",
    "\n",
    "IceEnv = Environment(start = Start, goal = End, length = M, height = N)\n",
    "\n",
    "dirs = {\n",
    "    'Up':    (-1,  0),\n",
    "    'Down':  ( 1,  0),\n",
    "    'Left':  ( 0, -1),\n",
    "    'Right': ( 0,  1),\n",
    "}\n",
    "for i in range(N):\n",
    "    for j in range(M):\n",
    "        if map[i][j] == 'x':\n",
    "            continue\n",
    "\n",
    "        for dir, (dy, dx) in dirs.items():\n",
    "            y = i\n",
    "            x = j\n",
    "            while y >= 0 and y < N and x >= 0 and x < M and map[y][x] != 'x':\n",
    "                y += dy\n",
    "                x += dx\n",
    "\n",
    "            y -= dy\n",
    "            x -= dx\n",
    "            if y != i or x != j:\n",
    "                IceEnv.GiveTransition(f'{i},{j}', dir, f'{y},{x}')\n",
    "                if map[y][x] == 'r':\n",
    "                    IceEnv.GiveValue(f'{i},{j}', dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Up': (-1, 0), 'Down': (1, 0), 'Left': (0, -1), 'Right': (0, 1)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Initialize Training\n",
    "alpha = 1\n",
    "gamma = 0.7\n",
    "epsilon = 0.8\n",
    "max_step = 500\n",
    "decay=0.95\n",
    "\n",
    "IceEnv.CreateQMat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.        24.01       0.         0.      ]\n",
      " [  0.        34.3        0.        16.807   ]\n",
      " [  0.         0.         0.        16.807   ]\n",
      " [  0.        11.7649    24.01      16.807   ]\n",
      " [  0.        11.7649    24.01       0.      ]\n",
      " [  0.         5.764801   0.       100.      ]\n",
      " [  0.        49.        70.       100.      ]\n",
      " [  0.         0.         0.         0.      ]\n",
      " [  0.         5.764801   0.        11.7649  ]\n",
      " [  0.         0.         8.23543    0.      ]\n",
      " [  8.23543   11.7649    16.807      0.      ]\n",
      " [  8.23543    0.         0.         0.      ]\n",
      " [ 70.         0.         0.        34.3     ]\n",
      " [ 24.01       0.        49.        34.3     ]\n",
      " [ 11.7649     0.        49.         0.      ]\n",
      " [ 16.807      0.         0.        11.7649  ]\n",
      " [ 16.807      0.        11.7649     0.      ]]\n",
      "0.004736423376267195\n",
      "{'0,1': 0, '0,3': 1, '0,4': 2, '0,6': 3, '0,8': 4, '1,1': 5, '1,2': 6, '1,9': 7, '3,0': 8, '3,1': 9, '3,6': 10, '4,0': 11, '4,2': 12, '4,3': 13, '4,4': 14, '4,6': 15, '4,8': 16}\n",
      "['(3, 0), Right', '(3, 6), Left', '(0, 6), Left', '(0, 3), Down', '(4, 3), Left', '(4, 2), Up', '(1, 2), Right'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    actionlist=IceEnv.TrainEpisode(alpha,gamma,epsilon,max_step)\n",
    "    epsilon*=decay\n",
    "\n",
    "print(IceEnv.Q)\n",
    "print(epsilon)\n",
    "print(IceEnv.S_dict)\n",
    "print('{} \\n'.format([a for a in actionlist]))\n",
    "# print('{} \\n'.format([a for a in actionlist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
