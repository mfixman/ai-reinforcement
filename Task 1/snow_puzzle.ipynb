{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice Puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](snow_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install packages\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Environment:\n",
    "    def __init__(self,totems,ice,treasure,start,goal,length=10,height=5,punishment=-5,reward=100):\n",
    "        # Define environment size\n",
    "        self.length=length\n",
    "        self.height=height\n",
    "        self.Map=np.array(np.zeros((height,length)))\n",
    "\n",
    "        # Define Items on the Map\n",
    "        self.start=start #Starting Point\n",
    "        self.goal=goal\n",
    "        self.totems=totems\n",
    "        self.ice=ice\n",
    "        self.treasure=treasure\n",
    "        self.punishment=punishment\n",
    "        self.reward=reward\n",
    "\n",
    "        # Define Variables for Training\n",
    "        self.actions_map = {\"Up\": 0,\"Down\": 1,\"Left\": 2,\"Right\": 3}\n",
    "        self.Q=None\n",
    "        self.R=None\n",
    "        self.S=None\n",
    "        self.episode_actions=[]\n",
    "\n",
    "        # IMPORTANT: self.S_dict IS NOT THE SAME VARIABLE AS self.S\n",
    "        # self.S_dict is a dictionary that takes in a coordinate and transforms it to an index corresponding to the element in self.S\n",
    "        self.S_dict={}\n",
    "\n",
    "        # Define the next state because this map cannot utilise simple directions\n",
    "        self.transitions=None\n",
    "\n",
    "    #Creating State Matrices and its corresponding index in self.S_dict\n",
    "    def CreateStateMat(self): \n",
    "        self.S=[]\n",
    "        self.S_dict={}\n",
    "        iter=0\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.length):\n",
    "                if((i,j) not in self.totems+self.ice):\n",
    "                    # self.S_dict.update({iter:(i,j)})\n",
    "                    self.S.append((i,j))\n",
    "                    self.S_dict.update({'{},{}'.format(i,j):iter})\n",
    "                    iter+=1\n",
    "\n",
    "        return self.S_dict\n",
    "\n",
    "    #Defining the Transition Matrix\n",
    "    def CreateTransitions(self): \n",
    "        self.transitions=np.full((len(self.R),len(self.actions)),np.nan)\n",
    "        return self.transitions\n",
    "\n",
    "    def GiveTransition(self,loc,act,destination):\n",
    "        # Variables:\n",
    "        # 1) loc: Coordinate in the form of 'x,y' INCLUDING the apostrophes, \n",
    "        # which converts to its corresponding index through self.s_dict\n",
    "        # 2) acts: Possible actions in the form of ['Up','Down,'Left','Right'] where any of the directions \n",
    "        # can be removed to indicate the action is not possible\n",
    "        # 3) destinations: End coordinate after taking the action defined above in the form of ['x,y']\n",
    "        # Shapes of acts and destinations must be the same\n",
    "\n",
    "        if(len(acts)!=len(destinations)):\n",
    "            raise Exception('Action List must be equal length to Destination List')\n",
    "\n",
    "        self.transitions[self.S_dict[loc],self.actions_map[act]] = self.S_dict[destinations]\n",
    "        self.R[self.S_dict[loc],self.actions_mao[act]] = 0\n",
    "\n",
    "        return\n",
    "\n",
    "    def CreateRMat(self):\n",
    "        self.R=np.full((len(self.S_dict),len(self.actions)),np.nan)\n",
    "        return self.R\n",
    "\n",
    "    def CreateQMat(self):\n",
    "        self.Q=np.zeros((len(self.S_dict),len(self.actions)))\n",
    "\n",
    "    #Assign reward value of punishment value\n",
    "    def GiveValue(self,loc,acts,value=None): \n",
    "        if value is None:\n",
    "            value = self.reward\n",
    "        self.R[self.S_dict[loc[0]],self.actions.index(acts)]=value\n",
    "\n",
    "    def TrainEpisode(self,alpha,gamma,epsilon,max_step,default_start=True): \n",
    "        #...\n",
    "        # Training Episodes using Code from INM707 Lab 4 as reference\n",
    "        #...\n",
    "        if(default_start==True):\n",
    "            curr_s=self.S_dict[self.start]\n",
    "        else:\n",
    "            curr_s=random.randint(0,len(self.S_dict)-1)\n",
    "        # print(\"Starting state is '{}'\".format(list(self.S_dict.keys())[curr_s]))\n",
    "        self.episode_actions=[]\n",
    "        for step in range(max_step):\n",
    "\n",
    "            # Define actions for both exploring and exploiting policies\n",
    "            open_actions = np.where(~np.isnan(self.R[curr_s]))[0]\n",
    "            # print([self.actions[a] for a in open_actions])\n",
    "\n",
    "            open_q = [self.Q[curr_s,a] for a in open_actions]\n",
    "\n",
    "            best_act = open_actions[np.where(open_q == np.max(open_q))[0]]\n",
    "            best_act_q = [self.Q[curr_s,x] for x in best_act]\n",
    "\n",
    "            # print(best_act)\n",
    "\n",
    "            # Pick Action based on policy\n",
    "            if np.random.uniform() < epsilon:\n",
    "                a = np.random.choice(open_actions)\n",
    "            else:\n",
    "                a = np.random.choice(best_act)\n",
    "\n",
    "            # Update Environment States\n",
    "            r = self.R[curr_s,a]\n",
    "            s_old = curr_s\n",
    "            curr_s = int(self.transitions[curr_s,a])\n",
    "\n",
    "            self.episode_actions.append(\"{}, {}\".format(self.S[s_old],self.actions[a]))\n",
    "            # print(\"New state is '{}'\".format(list(self.S_dict.keys())[int(curr_s)]))\n",
    "            # print((self.Q[curr_s]))\n",
    "            q_updated =  self.Q[s_old,a] + alpha*(self.R[s_old,a] + gamma*np.max(self.Q[curr_s]) - self.Q[s_old,a])\n",
    "            self.Q[s_old,a] = q_updated\n",
    "\n",
    "            # print('Q matrix updated: \\n\\n {}'.format(self.Q.round(0)))\n",
    "\n",
    "            if curr_s == self.S_dict[self.goal]:\n",
    "                # print(\"Goal state '{}' reached. Ending episode.\".format(self.goal))\n",
    "                break\n",
    "\n",
    "        return self.episode_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan   0.  nan   0.   0.  -1.   0.  -1.   0.  nan]\n",
      " [ nan   0.   0.  -1.  -1.  -1.  -1.  -1.  -1. 100.]\n",
      " [ nan  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  nan]\n",
      " [  0.   0.  -1.  -1.  -1.  -1.   0.  nan  -1.  nan]\n",
      " [  0.  nan   0.   0.   0.  nan   0.  -1.   0.  nan]]\n"
     ]
    }
   ],
   "source": [
    "#Define Totems (Tiles that cant be stood on), Ice (Tiles that can only be slid past), Treasure(Tile containing reward) and Start(Starting Point)\n",
    "Totems=[(0,0),(1,0),(2,0),(0,2),(4,1),(4,5),(3,7),(0,9),(2,9),(3,9),(4,9)]\n",
    "Ice=[(2,1),(2,2),(3,2),(1,3),(2,3),(3,3),(1,4),(2,4),(3,4),(0,5),(1,5),(2,5),(3,5),(2,7),(2,8),(1,6),(2,6),(0,7),(1,7),(4,7),(1,8),(3,8)]\n",
    "Treasure=[(1,9)]\n",
    "Start='3,0'\n",
    "End = '1,9']\n",
    "Start='3,0'\n",
    "End = '1,9'\n",
    "\n",
    "Map = IceEnv.CreateMap()\n",
    "print(Map)\n",
    "\n",
    "#Create State Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read map from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('snow_map').readlines()\n",
    "\n",
    "# Remove first line\n",
    "lines = lines[1:]\n",
    "\n",
    "map = []\n",
    "for line in lines:\n",
    "    map.append(line[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 0123456789\n",
    "0x x      x\n",
    "1x        r\n",
    "2         x\n",
    "3       x  \n",
    "4     x   x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(map)\n",
    "M = len(map[0])\n",
    "\n",
    "IceEnv = Environment(Totems,Ice,Treasure,Start,End)\n",
    "S = IceEnv.CreateStateMat()\n",
    "R=IceEnv.CreateRMat()\n",
    "IceEnv.CreateTransitions()\n",
    "\n",
    "dirs = {\n",
    "    'Up':    (-1,  0),\n",
    "    'Down':  ( 1,  0),\n",
    "    'Left':  ( 0, -1),\n",
    "    'Right': ( 0,  1),\n",
    "}\n",
    "for i in range(N):\n",
    "    for j in range(M):\n",
    "        if map[i][j] == 'x':\n",
    "            continue\n",
    "\n",
    "        for dir, (dy, dx) in dirs.items():\n",
    "            y = i\n",
    "            x = j\n",
    "            while y >= 0 and y < N and x >= 0 and x < M and map[y][x] != 'x':\n",
    "                y += dy\n",
    "                x += dx\n",
    "\n",
    "            y -= dy\n",
    "            x -= dx\n",
    "            if y != i and x != j:\n",
    "                IceEnv.GiveTransition(f'{i},{j}', dir, f'{y},{x}')\n",
    "                if map[y][x] == 'r':\n",
    "                    IceEnv.GiveValue(f'{i},{j}', dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (0, 6),\n",
       " (0, 8),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 9),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 6),\n",
       " (4, 0),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (4, 6),\n",
       " (4, 8)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IceEnv.S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Initialize Training\n",
    "alpha = 1\n",
    "gamma = 0.7\n",
    "epsilon = 0.8\n",
    "max_step = 500\n",
    "decay=0.95\n",
    "\n",
    "IceEnv.CreateQMat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.        24.01       0.         0.      ]\n",
      " [  0.        34.3        0.        16.807   ]\n",
      " [  0.         0.         0.        16.807   ]\n",
      " [  0.        11.7649    24.01      16.807   ]\n",
      " [  0.        11.7649    24.01       0.      ]\n",
      " [  0.         5.764801   0.       100.      ]\n",
      " [  0.        49.        70.       100.      ]\n",
      " [  0.         0.         0.         0.      ]\n",
      " [  0.         5.764801   0.        11.7649  ]\n",
      " [  0.         0.         8.23543    0.      ]\n",
      " [  8.23543   11.7649    16.807      0.      ]\n",
      " [  8.23543    0.         0.         0.      ]\n",
      " [ 70.         0.         0.        34.3     ]\n",
      " [ 24.01       0.        49.        34.3     ]\n",
      " [ 11.7649     0.        49.         0.      ]\n",
      " [ 16.807      0.         0.        11.7649  ]\n",
      " [ 16.807      0.        11.7649     0.      ]]\n",
      "0.004736423376267195\n",
      "{'0,1': 0, '0,3': 1, '0,4': 2, '0,6': 3, '0,8': 4, '1,1': 5, '1,2': 6, '1,9': 7, '3,0': 8, '3,1': 9, '3,6': 10, '4,0': 11, '4,2': 12, '4,3': 13, '4,4': 14, '4,6': 15, '4,8': 16}\n",
      "['(3, 0), Right', '(3, 6), Left', '(0, 6), Left', '(0, 3), Down', '(4, 3), Left', '(4, 2), Up', '(1, 2), Right'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    actionlist=IceEnv.TrainEpisode(alpha,gamma,epsilon,max_step)\n",
    "    epsilon*=decay\n",
    "\n",
    "print(IceEnv.Q)\n",
    "print(epsilon)\n",
    "print(IceEnv.S_dict)\n",
    "print('{} \\n'.format([a for a in actionlist]))\n",
    "# print('{} \\n'.format([a for a in actionlist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
