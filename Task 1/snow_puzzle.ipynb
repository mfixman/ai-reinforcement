{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice Puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](snow_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install packages\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self,start,goal,length=10,height=5,punishment=-5,reward=100):\n",
    "        # Define environment size\n",
    "        self.length=length\n",
    "        self.height=height\n",
    "        self.Map=np.array(np.zeros((height,length)))\n",
    "\n",
    "        # Define Items on the Map\n",
    "        self.start=start #Starting Point\n",
    "        self.goal=goal\n",
    "        self.punishment=punishment\n",
    "        self.reward=reward\n",
    "\n",
    "        # Define Variables for Training\n",
    "        self.actions_map = {\"Up\": 0,\"Down\": 1,\"Left\": 2,\"Right\": 3}\n",
    "        self.Q=None\n",
    "        self.transitions = dict()\n",
    "        self.R = dict()\n",
    "        self.S = set()\n",
    "        self.episode_actions=[]\n",
    "\n",
    "        # Define the next state because this map cannot utilise simple directions\n",
    "        self.transitions=None\n",
    "\n",
    "    def GiveTransition(self,loc,act,destination):\n",
    "        # Variables:\n",
    "        # 1) loc: Coordinate in the form of 'x,y' INCLUDING the apostrophes, \n",
    "        # which converts to its corresponding index through self.s_dict\n",
    "        # 2) acts: Possible actions in the form of ['Up','Down,'Left','Right'] where any of the directions \n",
    "        # can be removed to indicate the action is not possible\n",
    "        # 3) destinations: End coordinate after taking the action defined above in the form of ['x,y']\n",
    "        # Shapes of acts and destinations must be the same\n",
    "\n",
    "        self.transitions[loc,self.actions_map[act]] = destination\n",
    "        self.R[loc,self.actions_map[act]] = 0\n",
    "        self.S.insert(loc)\n",
    "\n",
    "    def CreateQMat(self):\n",
    "        self.Q=np.zeros((len(self.St),len(self.actions)))\n",
    "\n",
    "    #Assign reward value of punishment value\n",
    "    def GiveValue(self,loc,acts,value=None): \n",
    "        if value is None:\n",
    "            value = self.reward\n",
    "        self.R[loc,self.actions.index(acts)]=value\n",
    "\n",
    "    def TrainEpisode(self,alpha,gamma,epsilon,max_step,default_start=True): \n",
    "        #...\n",
    "        # Training Episodes using Code from INM707 Lab 4 as reference\n",
    "        #...\n",
    "        if(default_start==True):\n",
    "            curr_s=self.start\n",
    "        else:\n",
    "            curr_s=random.randint(0,len(self.S)-1)\n",
    "        # print(\"Starting state is '{}'\".format(list(self.S_dict.keys())[curr_s]))\n",
    "        self.episode_actions=[]\n",
    "        for step in range(max_step):\n",
    "\n",
    "            # Define actions for both exploring and exploiting policies\n",
    "            open_actions = np.where(~np.isnan(self.R[curr_s]))[0]\n",
    "            # print([self.actions[a] for a in open_actions])\n",
    "\n",
    "            open_q = [self.Q[curr_s,a] for a in open_actions]\n",
    "\n",
    "            best_act = open_actions[np.where(open_q == np.max(open_q))[0]]\n",
    "            best_act_q = [self.Q[curr_s,x] for x in best_act]\n",
    "\n",
    "            # print(best_act)\n",
    "\n",
    "            # Pick Action based on policy\n",
    "            if np.random.uniform() < epsilon:\n",
    "                a = np.random.choice(open_actions)\n",
    "            else:\n",
    "                a = np.random.choice(best_act)\n",
    "\n",
    "            # Update Environment States\n",
    "            r = self.R[curr_s,a]\n",
    "            s_old = curr_s\n",
    "            curr_s = int(self.transitions[curr_s,a])\n",
    "\n",
    "            self.episode_actions.append(\"{}, {}\".format(self.S[s_old],self.actions[a]))\n",
    "            # print(\"New state is '{}'\".format(list(self.S_dict.keys())[int(curr_s)]))\n",
    "            # print((self.Q[curr_s]))\n",
    "            q_updated =  self.Q[s_old,a] + alpha*(self.R[s_old,a] + gamma*np.max(self.Q[curr_s]) - self.Q[s_old,a])\n",
    "            self.Q[s_old,a] = q_updated\n",
    "\n",
    "            # print('Q matrix updated: \\n\\n {}'.format(self.Q.round(0)))\n",
    "\n",
    "            if curr_s == self.goal:\n",
    "                # print(\"Goal state '{}' reached. Ending episode.\".format(self.goal))\n",
    "                break\n",
    "\n",
    "        return self.episode_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from collections import defaultdict\n",
    "\n",
    "class Environment:\n",
    "    dirs = {\n",
    "        0: (-1,  0),\n",
    "        1: ( 1,  0),\n",
    "        2: ( 0, -1),\n",
    "        3: ( 0,  1),\n",
    "    }\n",
    "\n",
    "    def __init__(self, map):\n",
    "        self.parseMap(map)\n",
    "        self.reward_val = 100\n",
    "        self.Q = numpy.full((self.N, self.M, len(self.dirs)), numpy.nan)\n",
    "        for (ty, tx), tds in self.transitions.items():\n",
    "            for t in tds.keys():\n",
    "                self.Q[ty, tx, t] = 0\n",
    "\n",
    "    def parseMap(self, map):\n",
    "        self.map = map\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.rewards = set()\n",
    "        self.N = len(map)\n",
    "        self.M = len(map[0])\n",
    "\n",
    "        self.transitions = defaultdict(lambda: dict())\n",
    "\n",
    "        for y, row in enumerate(self.map):\n",
    "            for x, tile in enumerate(row):\n",
    "                self.processTile(y, x, tile)\n",
    "\n",
    "        if self.start is None:\n",
    "            raise ValueError('No start')\n",
    "        if self.end is None:\n",
    "            raise ValueError('No end')\n",
    "\n",
    "    def canStep(self, y, x):\n",
    "        return y >= 0 and y < self.N and x >= 0 and x < self.M and self.map[y][x] != 'x'\n",
    "\n",
    "    def processTile(self, y, x, tile):\n",
    "        if tile == 's':\n",
    "            if self.start is not None:\n",
    "                raise ValueError('Several starts')\n",
    "            self.start = (y, x)\n",
    "\n",
    "        if tile in ('e', 'R'):\n",
    "            if self.end is not None:\n",
    "                raise ValueError('Several ends')\n",
    "            self.end = (y, x)\n",
    "\n",
    "        if tile == 'x':\n",
    "            return\n",
    "\n",
    "        assert self.canStep(y, x)\n",
    "        for dir, (dy, dx) in self.dirs.items():\n",
    "            cy, cx = y, x\n",
    "            while self.canStep(cy, cx):\n",
    "                cy += dy\n",
    "                cx += dx\n",
    "            cy -= dy\n",
    "            cx -= dx\n",
    "\n",
    "            assert dir not in self.transitions[(y, x)]\n",
    "            self.transitions[(y, x)][dir] = (cy, cx)\n",
    "\n",
    "    def trainEpisode(self, alpha, gamma, epsilon, max_steps):\n",
    "        s = self.start\n",
    "\n",
    "        open_actions = self.transitions[s].keys()\n",
    "        best_actions = [x for x in self.dirs.keys() if self.Q[s][x] == numpy.max(self.Q[s])]\n",
    "        # open_values = numpy.array(self.transition[s].values())\n",
    "        for step in range(max_steps):\n",
    "            if numpy.random.uniform() < epsilon:\n",
    "                a = numpy.random.choice(open_actions)\n",
    "            else:\n",
    "                try:\n",
    "                    a = numpy.random.choice(best_actions)\n",
    "                except ValueError:\n",
    "                    print(f'At state {s}')\n",
    "                    assert False\n",
    "\n",
    "            ns = self.transitions[s][a]\n",
    "            r = self.reward_val if ns == self.end else 0\n",
    "            self.Q[s][a] += alpha * (r + gamma * numpy.max(self.Q[ns]) - self.Q[s][a])\n",
    "\n",
    "            s = ns\n",
    "            if s == self.end:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional or an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:941\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_keys' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m e \u001b[38;5;241m=\u001b[39m Environment(\u001b[38;5;28mmap\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainEpisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[144], line 76\u001b[0m, in \u001b[0;36mEnvironment.trainEpisode\u001b[0;34m(self, alpha, gamma, epsilon, max_steps)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform() \u001b[38;5;241m<\u001b[39m epsilon:\n\u001b[0;32m---> 76\u001b[0m         a \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopen_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:943\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional or an integer"
     ]
    }
   ],
   "source": [
    "e = Environment(map)\n",
    "e.trainEpisode(alpha = 1, gamma = 0.7, epsilon = 0.8, max_steps = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Up': (3, 0), 'Down': (4, 0), 'Left': (3, 0), 'Right': (3, 6)}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.transitions[(3, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.argmax(e.Q[3, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = numpy.array(\n",
    "    [\n",
    "        [ 1, 2, 3, 4 ],\n",
    "        [ 5, 6, 7, 8 ],\n",
    "        [ 9, 1, 2, 3 ],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.unravel_index(Q.argmax(), Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = numpy.array([(0, 1), (0, 2), (1, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 7])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[a[:, 0], a[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[numpy.argmax(Q[a[:, 0], a[:, 1]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]],\n",
       "\n",
       "       [[5, 6, 7, 8],\n",
       "        [9, 1, 2, 3]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[[(0, 1), (1, 2)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan   0.  nan   0.   0.  -1.   0.  -1.   0.  nan]\n",
      " [ nan   0.   0.  -1.  -1.  -1.  -1.  -1.  -1. 100.]\n",
      " [ nan  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  nan]\n",
      " [  0.   0.  -1.  -1.  -1.  -1.   0.  nan  -1.  nan]\n",
      " [  0.  nan   0.   0.   0.  nan   0.  -1.   0.  nan]]\n"
     ]
    }
   ],
   "source": [
    "#Define Totems (Tiles that cant be stood on), Ice (Tiles that can only be slid past), Treasure(Tile containing reward) and Start(Starting Point)\n",
    "Totems=[(0,0),(1,0),(2,0),(0,2),(4,1),(4,5),(3,7),(0,9),(2,9),(3,9),(4,9)]\n",
    "Ice=[(2,1),(2,2),(3,2),(1,3),(2,3),(3,3),(1,4),(2,4),(3,4),(0,5),(1,5),(2,5),(3,5),(2,7),(2,8),(1,6),(2,6),(0,7),(1,7),(4,7),(1,8),(3,8)]\n",
    "Treasure=[(1,9)]\n",
    "Start='3,0'\n",
    "End = '1,9']\n",
    "Start='3,0'\n",
    "End = '1,9'\n",
    "\n",
    "Map = IceEnv.CreateMap()\n",
    "print(Map)\n",
    "\n",
    "#Create State Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read map from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('snow_map').readlines()\n",
    "\n",
    "# Remove first line\n",
    "lines = lines[1:]\n",
    "\n",
    "map = []\n",
    "for line in lines:\n",
    "    map.append(line[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0123456789\n",
      "0x x      x\n",
      "1x        R\n",
      "2x        x\n",
      "3s      x x\n",
      "4 x   x   x\n"
     ]
    }
   ],
   "source": [
    "print(' ' + ''.join([str(x) for x in range(10)]))\n",
    "print('\\n'.join([f'{x}{y}' for x, y in enumerate(map)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m dx\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;241m!=\u001b[39m i \u001b[38;5;129;01mor\u001b[39;00m x \u001b[38;5;241m!=\u001b[39m j:\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mIceEnv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGiveTransition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mj\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43my\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmap\u001b[39m[y][x] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     29\u001b[0m         IceEnv\u001b[38;5;241m.\u001b[39mGiveValue(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mdir\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 34\u001b[0m, in \u001b[0;36mEnvironment.GiveTransition\u001b[0;34m(self, loc, act, destination)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGiveTransition\u001b[39m(\u001b[38;5;28mself\u001b[39m,loc,act,destination):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Variables:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# 1) loc: Coordinate in the form of 'x,y' INCLUDING the apostrophes, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# 3) destinations: End coordinate after taking the action defined above in the form of ['x,y']\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Shapes of acts and destinations must be the same\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransitions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mact\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m destination\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR[loc,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions_map[act]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS\u001b[38;5;241m.\u001b[39minsert(loc)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "N = len(map)\n",
    "M = len(map[0])\n",
    "\n",
    "IceEnv = Environment(start = '3,0', goal = '1,9', length = M, height = N)\n",
    "\n",
    "dirs = {\n",
    "    'Up':    (-1,  0),\n",
    "    'Down':  ( 1,  0),\n",
    "    'Left':  ( 0, -1),\n",
    "    'Right': ( 0,  1),\n",
    "}\n",
    "for i in range(N):\n",
    "    for j in range(M):\n",
    "        if map[i][j] == 'x':\n",
    "            continue\n",
    "\n",
    "        for dir, (dy, dx) in dirs.items():\n",
    "            y = i\n",
    "            x = j\n",
    "            while y >= 0 and y < N and x >= 0 and x < M and map[y][x] != 'x':\n",
    "                y += dy\n",
    "                x += dx\n",
    "\n",
    "            y -= dy\n",
    "            x -= dx\n",
    "            if y != i or x != j:\n",
    "                IceEnv.GiveTransition(f'{i},{j}', dir, f'{y},{x}')\n",
    "                if map[y][x] == 'r':\n",
    "                    IceEnv.GiveValue(f'{i},{j}', dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Up': (-1, 0), 'Down': (1, 0), 'Left': (0, -1), 'Right': (0, 1)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Initialize Training\n",
    "alpha = 1\n",
    "gamma = 0.7\n",
    "epsilon = 0.8\n",
    "max_step = 500\n",
    "decay=0.95\n",
    "\n",
    "IceEnv.CreateQMat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.        24.01       0.         0.      ]\n",
      " [  0.        34.3        0.        16.807   ]\n",
      " [  0.         0.         0.        16.807   ]\n",
      " [  0.        11.7649    24.01      16.807   ]\n",
      " [  0.        11.7649    24.01       0.      ]\n",
      " [  0.         5.764801   0.       100.      ]\n",
      " [  0.        49.        70.       100.      ]\n",
      " [  0.         0.         0.         0.      ]\n",
      " [  0.         5.764801   0.        11.7649  ]\n",
      " [  0.         0.         8.23543    0.      ]\n",
      " [  8.23543   11.7649    16.807      0.      ]\n",
      " [  8.23543    0.         0.         0.      ]\n",
      " [ 70.         0.         0.        34.3     ]\n",
      " [ 24.01       0.        49.        34.3     ]\n",
      " [ 11.7649     0.        49.         0.      ]\n",
      " [ 16.807      0.         0.        11.7649  ]\n",
      " [ 16.807      0.        11.7649     0.      ]]\n",
      "0.004736423376267195\n",
      "{'0,1': 0, '0,3': 1, '0,4': 2, '0,6': 3, '0,8': 4, '1,1': 5, '1,2': 6, '1,9': 7, '3,0': 8, '3,1': 9, '3,6': 10, '4,0': 11, '4,2': 12, '4,3': 13, '4,4': 14, '4,6': 15, '4,8': 16}\n",
      "['(3, 0), Right', '(3, 6), Left', '(0, 6), Left', '(0, 3), Down', '(4, 3), Left', '(4, 2), Up', '(1, 2), Right'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    actionlist=IceEnv.TrainEpisode(alpha,gamma,epsilon,max_step)\n",
    "    epsilon*=decay\n",
    "\n",
    "print(IceEnv.Q)\n",
    "print(epsilon)\n",
    "print(IceEnv.S_dict)\n",
    "print('{} \\n'.format([a for a in actionlist]))\n",
    "# print('{} \\n'.format([a for a in actionlist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
