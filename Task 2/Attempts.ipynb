{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edb279e-cf8c-4f8a-99f4-c40eefc1de7d",
   "metadata": {},
   "source": [
    "# Prelude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6c09cc-b457-40ef-8a10-97e6ef4791d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy\n",
    "import matplotlib\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from collections import deque, namedtuple\n",
    "from itertools import count\n",
    "from numpy import ndarray\n",
    "from gymnasium import spaces\n",
    "from numpy import random\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258a4dfd-6d53-47a5-8b27-b84ee4d7705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86dffac-5026-4024-9c23-3314bd6f1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab04d0-9cab-4629-a177-74da8afd06d1",
   "metadata": {},
   "source": [
    "# Skating Rink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e15fcfb-01b2-4126-b62b-e524b8448bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = -1\n",
    "none = 0\n",
    "right = 1\n",
    "\n",
    "speed = .1\n",
    "ang_speed = 1/10 * (2 * numpy.pi)\n",
    "\n",
    "end = numpy.array([10, 10])\n",
    "\n",
    "class SkatingRinkEnv(gym.Env):\n",
    "    state : ndarray\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SkatingRinkEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(3)  # Left and right actions\n",
    "        self.observation_space = spaces.Box(low=numpy.array([0, 0, -numpy.pi]), high=numpy.array([100, 100, numpy.pi]), dtype=numpy.float32)  # Example bounds\n",
    "\n",
    "        # Initialize state\n",
    "        self.state = numpy.zeros(3)\n",
    "\n",
    "    def step(self, action : int) -> tuple[ndarray, float, bool, dict]:\n",
    "        self.state = numpy.array([\n",
    "            self.state[0] + action * speed * numpy.sin(self.state[2]),\n",
    "            self.state[1] + action * speed * numpy.cos(self.state[2]),\n",
    "            self.state[2] + ang_speed * action,\n",
    "        ])\n",
    "\n",
    "        coords = self.state[[0, 1]]\n",
    "        done = numpy.sqrt(numpy.sum((coords - end) ** 2)) < 1\n",
    "        reward = 1 if done else -0.01\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self) -> ndarray:\n",
    "        self.state = numpy.zeros(3)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc679967-483d-41e3-b5e6-4355dcc297ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(64, 64)         # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(64, output_dim) # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Activation function for first layer\n",
    "        x = F.relu(self.fc2(x))  # Activation function for second layer\n",
    "        x = self.fc3(x)          # No activation function for output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f0e715-2ff9-41c9-a40c-97e8b0ccae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Sean\\anaconda3\\envs\\INM705\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "env = SkatingRinkEnv()\n",
    "model = DQN(env.observation_space.shape[0], output_dim = env.action_space.n)\n",
    "target_model = DQN(env.observation_space.shape[0], output_dim = env.action_space.n)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49eb85dc-45ba-4eb7-8ad7-79e7adfa37dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41c8e0c-a40e-42aa-875f-20ee4d952177",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1  # Total episodes\n",
    "gamma = 0.99  # Discount factor for future rewards\n",
    "epsilon_start = 1.0  # Starting value of epsilon\n",
    "epsilon_end = 0.01  # Minimum value of epsilon\n",
    "epsilon_decay = 500  # Decay rate of epsilon\n",
    "batch_size = 64\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "target_update = 10  # Update the target network every 10 episodes\n",
    "\n",
    "# Decay function for epsilon\n",
    "def epsilon_by_episode(episode):\n",
    "    return epsilon_end + (epsilon_start - epsilon_end) * numpy.exp(-1. * episode / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf481904-4335-4ce5-a84e-10812a8e4ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't get to finish!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    epsilon = epsilon_by_episode(episode)\n",
    "    episode_rewards = 0\n",
    "\n",
    "    for e in range(0, 10000):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                action = model(state_tensor).argmax().item()\n",
    "\n",
    "        # Take action and observe the next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        episode_rewards += reward\n",
    "\n",
    "        # Start learning once the replay buffer has enough samples\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            batch = replay_buffer.sample(batch_size)\n",
    "            batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*batch)\n",
    "\n",
    "            batch_states = torch.FloatTensor(numpy.array(batch_states))\n",
    "            batch_actions = torch.LongTensor(numpy.array(batch_actions))\n",
    "            batch_rewards = torch.FloatTensor(numpy.array(batch_rewards))\n",
    "            batch_next_states = torch.FloatTensor(numpy.array(batch_next_states))\n",
    "            batch_dones = torch.FloatTensor([float(x) for x in numpy.array(batch_dones)])\n",
    "\n",
    "            # Compute the current Q values\n",
    "            current_q = model(batch_states).gather(1, batch_actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # Compute the next Q values using the target network\n",
    "            next_q = model(batch_next_states).max(1)[0]\n",
    "            expected_q = batch_rewards + gamma * next_q * (1 - batch_dones)\n",
    "\n",
    "            # Compute the loss and update the network\n",
    "            loss = loss_fn(current_q, expected_q.detach())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode: {episode}, Total Reward: {episode_rewards}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"Didn't get to finish!\")\n",
    "\n",
    "    # Update the target network\n",
    "    if episode % target_update == 0:\n",
    "        # Assuming you have a target network called target_model\n",
    "        target_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d6833-ca5f-4077-a95a-859aae73ad76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994ef05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
