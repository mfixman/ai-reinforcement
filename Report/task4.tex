\renewcommand{\thesection}{Extra Task}
\section{PPO Implementation}

For this section we attempt to implement Proximal Policy Optimization (PPO) where it deviates from the standard value-iteration calculations of the q-values used from techniques in the tasks above, and instead implements a policy gradient-based calculation. It implements a policy network and a value network. 

The PPO algorithm directly modifies the policy instead of the conventional state-action pairs obtained through the DQN methods. It utilises entropy and interactions with the environment to update the best policy for the agent to carry out to best fit the environment.

For the implementation, the code is referenced from the labs and also mildly inspired from the Atari PPO implementation\cite{ppo_github}, the environment used for this implementation is Space Invaders. The environment is modified to provide batches of input determined by batch size, followed by preprocessing techniques such as grayscaling and resizing to dimensions $84 \times 84$.

To learn the RGB image of the environment, the model is modified from taking discrete observation spaces as input to instead take in images through the addition of convolutional layers, flattened into dense layers to return an output. The output layers meanwhile, are modified to return two outputs, the predicted action determined by a softmax layer, followed by a value output to be calculated in the loss function.

The loss function is calculated as follows:
\begin{align*}
   L(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) - \beta H(\pi_{\theta}) \right] 
\end{align*}

\subsection{Results}
Implementation of the PPO algorithm on Space Invaders sees a max reward of around 600 from a measly 105 over 600 episodes. This is a significant improvement over regular DQN methods.
