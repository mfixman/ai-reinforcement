\renewcommand{\thesection}{Advanced Task}
\section{Deep Ice Skating using Deep Q-Learning}
\subsection{Environment}
The environment used for the Basic Task is too basic to implement Deep Q Learning. Instead, we created a more complicated ice rink environment, taking inspiration from the  same concept of the environment used in the Basic Task.

The agent is now located in a circular ice skating rink dojo which contains non-integer locations.
They are also using real ice skates that turn at different directions.

When skating, the state consists of 4 different floating point values and the state has 3 different actions.

\begin{center}
	\begin{minipage}[t]{.48\textwidth}
		\begin{center}
			\textbf{States}
		\end{center}
		\begin{description}[noitemsep,style=nextline]
			\item[\bm{$y$}] The $y$ coordinate of the agent.
			\item[\bm{$x$}] The $x$ coordinate of the agent.
			\item[\bm{$\varphi$}] The current angular the agent is at.
			\item[\bm{$\theta$}] The current angular velocity.
		\end{description}
	\end{minipage}
	\begin{minipage}[t]{.48\textwidth}
		\begin{center}
			\textbf{Actions}
		\end{center}
		\begin{description}[noitemsep]
			\item[0. Turn Left] Decrease angular velocity.
			\item[1. Stay Put] Keep angular velocity.
			\item[2. Turn Right] Increase angular velocity.
		\end{description}
	\end{minipage}
\end{center}

An agent starts at a random point of the ice dojo, and its objective is to get to the center as fast as possible.
This dojo is finite: if the agent goes far enough, it will fall off the flat earth and the game is terminated.

The state and transitions functions are defined in the following equation.
\newcommand{\statet}{\left< y, x, \varphi, \theta \right>}
\begin{gather*}
	\state = \statet \\
	\begin{aligned}
		y'(\state, a) &= y + \frac{1}{4} \sin(\varphi'(\state, a)) \\
		x'(\state, a) &= x + \frac{1}{4} \cos(\varphi'(\state, a)) \\
		\varphi'(\state, a) &= \varphi + \theta'(\state, a)
	\end{aligned}
	\qquad
	\theta'(\state, a)= \begin{cases}
		\theta^t - \frac{1}{10} \pi & \text{if } a = 0 \\
		\theta^t & \text{if } a = 1 \\
		\theta^t + \frac{1}{10} \pi & \text{if } a = 2
	\end{cases} \\[1ex]
	T(\state, a) = \left< y'(\state, a), x'(\state, a), \varphi'(\state, a), \theta'(\state, a) \right>
\end{gather*}

The reward function is defined to maximise the changes of an agent making it to the centre by, in addition to giving high scores for winning and low scores for losing, giving it a score depending on the distance to the center.
\begin{equation*}
	\reward(\state) = \begin{cases}
		\phantom{+}1000 & \text{if } \sqrt{y^2 + x^2} \leq \text{min distance} \\
		-1000 & \text{if } \sqrt{y^2 + x^2} \geq \text{max distance} \\[1ex]
		-\sqrt{y^2 + x^2} & \text{otherwise}
	\end{cases}
\end{equation*}

Adding a reward that depends on the distance considerably improved training speed and precision of the Deep Q-network compared to just subtracting a constant each time.
Even though the results ``should'' be the same, subtracting the distance disincentives the agent from attempting bad solutions.

\subsection{Implementation}
We implement a 3-layer Deep Q-network with hidden size set as a hyperparameter that's trained using an epsilon-greedy policy.
The data used to train the model is accumulated and later randomly sampled from a large replay buffer.

The original network (target = DQN) produces good solutions if trained in large batch sizes for a large amount of episodes.
Frustratingly, it takes a very long time to even make ``passable'' solutions.

In order to improve these convergence time, we present two improvements that can be alternatively added to this model.


\subsection{Improvements}
For the improvements we have implemented two algorithms, mainly the target network method and the
Double-DQN method. Both of the methods vary slightly in the implementation algorithm, but may have
impacting results if implemented right.
\subsubsection{Target Network}
The target network method introduces a new variable called the target network. The target network is initialised as an exact copy of the main network otherwise used in the conventional DQN method, but instead of updating every timestep, the target network is instead updated every certain timesteps, denoted by the variable update frequency, hence causing the target network to always lag behind the main network in terms of training.
The target network in this method evaluates the action made by the main network, calculating the best action and its Q-values using the formula:
\begin{align*}
    Q_{\text{target}}(s, a ; \theta^{-}) = r + \gamma \max_{a'} Q(s', a' ; \theta^{-})
\end{align*}
Evaluating the actions of the main network using the target network aims to increase the stability of the
overall training of the agent.

\subsection{Double DQN}

The double-DQN method (DDQN) similarly utilises the target network used by the target method method, but it differs in the sense that instead of getting the best actions and its q-values like the aforementioned two methods, the DDQN method gets the best action through the main network, but calculates the q-values using the target network, using the following formula:
\begin{align*}
    Q^{\text{DDQN}}(s, a ; \theta) = \mathbb{E}_{s' \sim \varepsilon}[r + \gamma Q(s', \underset{a'}{\text{argmax}}\ Q(s', a ; \theta_{\text{target}}) ; \theta)]
\end{align*}
By doing so, the DDQN method aims to mitigate overestimation bias problems from the traditional DQN method, where the agent tends to overestimate the Q-values obtained during training. By doing so, DDQN methods generally tend to converge faster than the DQN method due to the agent not being stuck at repeating suboptimal actions that do not yield the best results but appeared in the first few episodes.

\subsection{Results}

The following table presents the best 2 results for each method:

\begin{table}[h]
	\centering
	\scriptsize
	\begin{tabular}{r r r r | r r r r r r}
		\toprule
  method & hidden size & lr & gamma & eps start & win rate & best episode & best dones & loss & q step \\
  \midrule
  DQN & 256 & 0.001 & 0.99 & 0.5 & 1 & 178 & 1000 & 10030k & 7091.87 \\
  DQN & 512 & 0.001 & 0.99 & 0.8 & 1 & 197 & 1000 & 8306k & 4608.34 \\
  Target Network & 512 & 0.001 & 0.99 & 0.8 & 1 & 94 & 1000 & 17494k & 6114.58 \\
  Target Network & 512 & 0.001 & 0.99 & 0.5 & 1 & 124 & 1000 & 12819k & 6709.36 \\
  DDQN & 512 & 0.001 & 0.99 & 1 & 1 & 101 & 1000 & 22431k & 4119.00 \\
  DDQN & 512 & 0.001 & 0.99 & 0.8 & 1 & 107 & 1000 & 13977k & 7215.91 \\
  \bottomrule
  \end{tabular}
	\caption{Results using double DQN.}
	\label{best_results_t2}
\end{table}


\subsubsection{Observations}
Observing table 3, both of the Target Network and DDQN method outperforms the DQN method in terms of convergence speeds. This is because both methods were able to mitigate the overestimation of the Q-values compared to the DQN method through the target network.

\subsubsection{Hidden sizes}
Generally, the hidden size of 512 is preferred over 256. This proves that a higher hidden size allows agents to have more dimensionality in computing the best actions in the environment, leading the agent to be able to accomodate for a higher variety of actions. Care should be taken however, that too high of a hidden size will cause the model to overfit, which fortunately is not the case in this project.

\subsubsection{Gamma values}
For all cases, gamma = 0.99 is preferred over 0.9. The higher gamma value prioritises future rewards, while the lower gamma values value immediate rewards higher. For the advanced task, despite the complexity of the environment, the environment is still considered deterministic. In which case, since the agent is able to accurately calculate the rewards, higher gamma values allow the agent to learn the environment more thoroughly. However, in stochastic environments, a lower gamma value may be preferred due to the unpredictability of the agent when picking an action.

\subsection{Best Results}
