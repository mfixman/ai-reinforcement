\renewcommand{\thesection}{Advanced Task}
\section{Deep Ice Skating using Deep Q-Learning}
\subsection{Environment}
The environment used for the Basic Task is too basic to implement Deep Q Learning. Instead, we created a more complicated ice rink environment, taking inspiration from the  same concept of the environment used in the Basic Task.

The agent is now located in a circular ice skating rink dojo which contains non-integer locations.
They are also using real ice skates that turn at different directions.

When skating, the state consists of 4 different floating point values and the state has 3 different actions.

\begin{center}
	\begin{minipage}[t]{.48\textwidth}
		\begin{center}
			\textbf{States}
		\end{center}
		\begin{description}[noitemsep,style=nextline]
			\item[\bm{$y$}] The $y$ coordinate of the agent.
			\item[\bm{$x$}] The $x$ coordinate of the agent.
			\item[\bm{$\varphi$}] The current angular the agent is at.
			\item[\bm{$\theta$}] The current angular velocity.
		\end{description}
	\end{minipage}
	\begin{minipage}[t]{.48\textwidth}
		\begin{center}
			\textbf{Actions}
		\end{center}
		\begin{description}[noitemsep]
			\item[0. Turn Left] Decrease angular velocity.
			\item[1. Stay Put] Keep angular velocity.
			\item[2. Turn Right] Increase angular velocity.
		\end{description}
	\end{minipage}
\end{center}

An agent starts at a random point of the ice dojo, and its objective is to get to the center as fast as possible.
This dojo is finite: if the agent goes far enough, it will fall off the flat earth and the game is terminated.

The state and transitions functions are defined in the following equation.
\newcommand{\statet}{\left< y, x, \varphi, \theta \right>}
\begin{gather*}
	\state = \statet \\
	\begin{aligned}
		y'(\state, a) &= y + \frac{1}{4} \sin(\varphi'(\state, a)) \\
		x'(\state, a) &= x + \frac{1}{4} \cos(\varphi'(\state, a)) \\
		\varphi'(\state, a) &= \varphi + \theta'(\state, a)
	\end{aligned}
	\qquad
	\theta'(\state, a)= \begin{cases}
		\theta^t - \frac{1}{10} \pi & \text{if } a = 0 \\
		\theta^t & \text{if } a = 1 \\
		\theta^t + \frac{1}{10} \pi & \text{if } a = 2
	\end{cases} \\[1ex]
	T(\state, a) = \left< y'(\state, a), x'(\state, a), \varphi'(\state, a), \theta'(\state, a) \right>
\end{gather*}

The reward function is defined to maximise the changes of an agent making it to the centre by, in addition to giving high scores for winning and low scores for losing, giving it a score depending on the distance to the center.
\begin{equation*}
	\reward(\state) = \begin{cases}
		\phantom{+}1000 & \text{if } \sqrt{y^2 + x^2} \leq \text{min distance} \\
		-1000 & \text{if } \sqrt{y^2 + x^2} \geq \text{max distance} \\[1ex]
		-\sqrt{y^2 + x^2} & \text{otherwise}
	\end{cases}
\end{equation*}

Adding a reward that depends on the distance considerably improved training speed and precision of the Deep Q-network compared to just subtracting a constant each time.
Even though the results ``should'' be the same, subtracting the distance disincentives the agent from attempting bad solutions.

\newpage{}
\subsection{Implementation}
We implement a 3-layer Deep Q-network with hidden size set as a hyperparameter that's trained using an epsilon-greedy policy whose $\varepsilon$ value goes down linearly from a certain $\varepsilon$-start to 0.
The data used to train the model is accumulated and later randomly sampled from a large replay buffer.

The original network (target = DQN) produces good solutions if trained in large batch sizes for a large amount of episodes.
Frustratingly, it takes a very long time to even make ``passable'' solutions.

In order to improve these convergence time, we present two improvements that can be alternatively added to this model.


\subsection{Improvements}
For the improvements we have implemented two algorithms, mainly the target network method and the
Double-DQN method. Both of the methods vary slightly in the implementation algorithm, but may have
impacting results if implemented right.
\subsubsection{Target Network}
The target network method introduces a new variable called the target network. The target network is initialised as an exact copy of the main network otherwise used in the conventional DQN method, but instead of updating every timestep, the target network is instead updated every certain timesteps, denoted by the variable update frequency, hence causing the target network to always lag behind the main network in terms of training.
The target network in this method evaluates the action made by the main network, calculating the best action and its Q-values using the formula:
\begin{align*}
    Q_{\text{target}}(s, a ; \theta^{-}) = r + \gamma \max_{a'} Q(s', a' ; \theta^{-})
\end{align*}
Evaluating the actions of the main network using the target network aims to increase the stability of the
overall training of the agent.

\subsubsection{Double DQN}

The double-DQN method (DDQN) similarly utilises the target network used by the target method method, but it differs in the sense that instead of getting the best actions and its q-values like the aforementioned two methods, the DDQN method gets the best action through the main network, but calculates the q-values using the target network, using the following formula:
\begin{align*}
    Q^{\text{DDQN}}(s, a ; \theta) = \mathbb{E}_{s' \sim \varepsilon}[r + \gamma Q(s', \underset{a'}{\text{argmax}}\ Q(s', a ; \theta_{\text{target}}) ; \theta)]
\end{align*}
By doing so, the DDQN method aims to mitigate overestimation bias problems from the traditional DQN method, where the agent tends to overestimate the Q-values obtained during training. By doing so, DDQN methods generally tend to converge faster than the DQN method due to the agent not being stuck at repeating suboptimal actions that do not yield the best results but appeared in the first few episodes.

\subsection{Analysis of Results}

\Cref{best_results_t2} presents the best 2 results for each method, which can be used to choose candidates.
\begin{table}[h]
	\centering
	\scriptsize
	\begin{tabular}{r r r r r r | r r r r}
		\toprule
		ID & method & hidden size & lr & gamma & $\varepsilon$-start & win rate & best episode & loss & q step \\
  \midrule
		1 & DQN & 256 & 0.001 & 0.99 & 0.5 & 1 & 178 & 10030k & 7091.87 \\
		2 & DQN & 512 & 0.001 & 0.99 & 0.8 & 1 & 197 & 8306k & 4608.34 \\
		3 & Target Network & 512 & 0.001 & 0.99 & 0.8 & 1 & 94 & 17494k & 6114.58 \\
		4 & Target Network & 512 & 0.001 & 0.99 & 0.5 & 1 & 124 & 12819k & 6709.36 \\
		5 & DDQN & 512 & 0.001 & 0.99 & 1 & 1 & 101 & 22431k & 4119.00 \\
		6 & DDQN & 512 & 0.001 & 0.99 & 0.8 & 1 & 107 & 13977k & 7215.91 \\
  \bottomrule
  \end{tabular}
	\caption{Candidate networks which will be plotted in \cref{candidate_comparison}.}
	\label{best_results_t2}
\end{table}

\subsubsection{Observations}
Observing table 3, both of the Target Network and DDQN method outperforms the DQN method in terms of convergence speeds. This is because both methods were able to mitigate the overestimation of the Q-values compared to the DQN method through the target network.

\subsubsection{Hidden sizes}
Generally, the hidden size of 512 is preferred over 256. This proves that a higher hidden size allows agents to have more dimensionality in computing the best actions in the environment, leading the agent to be able to accomodate for a higher variety of actions. Care should be taken however, that too high of a hidden size will cause the model to overfit, which fortunately is not the case in this project.

\subsubsection{Gamma values}
For all cases, gamma = 0.99 is preferred over 0.9. The higher gamma value prioritises future rewards, while the lower gamma values value immediate rewards higher. For the advanced task, despite the complexity of the environment, the environment is still considered deterministic. In which case, since the agent is able to accurately calculate the rewards, higher gamma values allow the agent to learn the environment more thoroughly. However, in stochastic environments, a lower gamma value may be preferred due to the unpredictability of the agent when picking an action.

\subsection{Candidate Comparison}
\label{candidate_comparison}

We made further experiments to the results in \cref{best_results_t2} to find patterns by running several trials in validation sets of 500 episodes each.
The trials are averaged and the first 300 episodes are shown, since nothing interesting happens afterwards.
Note that the validation sets are agents independent of training; their results are never random regardless of the value of $\varepsilon$ at that episode.

\Cref{avg_reward} shows the average reward for each one of the models by epoch.
As expected, the DQN values in \cref{dqn_avg_reward} are extremely chaotic: while both models converge to having a high reward, the average at every episode seems random.
This is due to the innate instability of DQN\cite{reinforcement_learning_introduction}.

In contrast, \cref{rest_avg_reward} shows that the average rewards in both target network and DDQN models are much better behaved with all models reaching a perfect score quickly and not moving from there.
For this model TargetNetwork seems to converge much faster than DDQN.

\begin{figure}[h]
	\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{dqn_reward_size.png}
		\caption{Models using DQN.}
		\label{dqn_avg_reward}
	\end{subfigure} \\[1ex]
	\begin{subfigure}{\textwidth}
		\includegraphics[width=\textwidth]{rest_reward_size.png}
		\caption{Models using DDQN and Target Network.}
		\label{rest_avg_reward}
	\end{subfigure}
	\caption{Reward sizes for the models presented in \cref{best_results_t2}.}
	\label{avg_reward}
\end{figure}

As seem in \cref{losses}, the loss of all models goes down in time except for the DDQN with $\varepsilon\text{-start} = 1.0$, whose loss shoots up at around episode 75 and only comes down when the model converges.
The cause for this effect is difficult to find out, but it's possible that the regular network is stuck on a spot where agents lose almost instantanrously, while 
\begin{figure}[h]
	\includegraphics[width=\textwidth]{losses.png}
	\caption{Average loss of each model at each episode. The reason of the spike in loss in model ID 4 is hard to tell.}
	\label{losses}
\end{figure}

\subsection{Qualitative Analysis}

However, what does it mean for a candidate to be better than another?
