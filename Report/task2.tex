\renewcommand{\thesection}{Advanced Task}
\section{Deep Ice Skating using Deep Q-Learning}
\subsection{Environment}
The environment used for the Basic Task is too basic to implement Deep Q Learning. Instead, we created a more complicated ice rink environment, taking inspiration from the  same concept of the environment used in the Basic Task.

The agent is now located in a circular ice skating rink dojo which contains non-integer locations.
They are also using real ice skates that turn at different directions.

When skating, the state consists of 4 different floating point values and the state has 3 different actions.

\begin{center}
	\begin{minipage}[t]{.48\textwidth}
		\begin{center}
			\textbf{States}
		\end{center}
		\begin{description}[noitemsep,style=nextline]
			\item[\bm{$y$}] The $y$ coordinate of the agent.
			\item[\bm{$x$}] The $x$ coordinate of the agent.
			\item[\bm{$\varphi$}] The current angular the agent is at.
			\item[\bm{$\theta$}] The current angular velocity.
		\end{description}
	\end{minipage}
	\begin{minipage}[t]{.48\textwidth}
		\begin{center}
			\textbf{Actions}
		\end{center}
		\begin{description}[noitemsep]
			\item[0. Turn Left] Decrease angular velocity.
			\item[1. Stay Put] Keep angular velocity.
			\item[2. Turn Right] Increase angular velocity.
		\end{description}
	\end{minipage}
\end{center}

An agent starts at a random point of the ice dojo, and its objective is to get to the center as fast as possible.
This dojo is finite: if the agent goes far enough, it will fall off the flat earth and the game is terminated.

The state and transitions functions are defined in the following equation.
\newcommand{\statet}{\left< y, x, \varphi, \theta \right>}
\begin{gather*}
	\state = \statet \\
	\begin{aligned}
		y'(\state, a) &= y + \frac{1}{4} \sin(\varphi'(\state, a)) \\
		x'(\state, a) &= x + \frac{1}{4} \cos(\varphi'(\state, a)) \\
		\varphi'(\state, a) &= \varphi + \theta'(\state, a)
	\end{aligned}
	\qquad
	\theta'(\state, a)= \begin{cases}
		\theta^t - \frac{1}{10} \pi & \text{if } a = 0 \\
		\theta^t & \text{if } a = 1 \\
		\theta^t + \frac{1}{10} \pi & \text{if } a = 2
	\end{cases} \\[1ex]
	T(\state, a) = \left< y'(\state, a), x'(\state, a), \varphi'(\state, a), \theta'(\state, a) \right>
\end{gather*}

The reward function is defined to maximise the changes of an agent making it to the centre by, in addition to giving high scores for winning and low scores for losing, giving it a score depending on the distance to the center.
\begin{equation*}
	\reward(\state) = \begin{cases}
		\phantom{+}1000 & \text{if } \sqrt{y^2 + x^2} \leq \text{min distance} \\
		-1000 & \text{if } \sqrt{y^2 + x^2} \geq \text{max distance} \\[1ex]
		-\sqrt{y^2 + x^2} & \text{otherwise}
	\end{cases}
\end{equation*}

Adding a reward that depends on the distance considerably improved training speed and precision of the Deep Q-network compared to just subtracting a constant each time.
Even though the results ``should'' be the same, subtracting the distance disincentives the agent from attempting bad solutions.

\subsection{Implementation}
We implement a 3-layer Deep Q-network with hidden size set as a hyperparameter that's trained using an epsilon-greedy policy.
The data used to train the model is accumulated and later randomly sampled from a large replay buffer.

The original network (target = DQN) produces good solutions if trained in large batch sizes for a large amount of episodes.
Frustratingly, it takes a very long time to even make ``passable'' solutions.

In order to improve these convergence time, we present two improvements that can be alternatively added to this model.

\textbf{Target Network}

We introduce a new model, the target network.
This is initialised to an exact copy of the main neural network.

The target network 

For the improvements we have implemented two algorithms, mainly the target network method and the Double-DQN method.
Both of the methods vary slightly in the implementation algorithm, but may have impacting results if implemented right.
Both of the improvement methods introduce a new variable called the target network, which is initialised as an exact copy of the main network otherwise used in the conventional DQN method.
The difference lies in the target network method directly evaluating the action made by the main network, while the DDQN method instead evaluates the next action made by the main network in the DQN method.
By doing so, the target network helps to stabilise the convergence of the DQN network, while the DDQN method aims to prevent the DQN network from overestimating the Q-values, which may lead to overfitting when exploiting, where the agent thinks that it is doing the best move based on past results, which may be deterimental in stochastic environments where the same state-action pairs do not necessarily contribute to the same next state spaces.

